{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def splitDataToTrainAndTest(data_df,train_frac):\n",
    "    from sklearn.cross_validation import train_test_split\n",
    "    train = data_df.sample(frac=train_frac,random_state=1)\n",
    "    test = data_df.loc[~data_df.index.isin(train.index)]\n",
    "    return train,test\n",
    "def convertToBelegung(normalized_value,max_value):\n",
    "    return ((normalized_value - 0.01)/.99)*max_value\n",
    "\n",
    "class FullDataSet:\n",
    "    def __init__(self):\n",
    "        self.test = DataSet()\n",
    "        self.training = DataSet()\n",
    "    def setTrain_input(self,data):\n",
    "        self.training.inputData = data\n",
    "    def setTrain_output(self,data):\n",
    "        self.training.outputData = data\n",
    "    def setTest_input(self,data):\n",
    "        self.test.inputData = data\n",
    "    def setTest_output(self,data):\n",
    "        self.test.outputData = data\n",
    "    def getNumberInputs(self):\n",
    "        return self.test.inputData.shape[1]\n",
    "class DataSet:\n",
    "    def __init__(self):\n",
    "        self.inputData = []\n",
    "        self.outputData = []\n",
    "    def next_batch(self,batch_size):\n",
    "        b_in = self.inputData[np.random.choice(self.inputData.shape[0],batch_size,replace=False),:]\n",
    "        b_out = self.outputData[np.random.choice(self.outputData.shape[0],batch_size, replace=False),:]\n",
    "        return b_in,b_out\n",
    "    def num_examples(self):\n",
    "        return self.inputData.shape[0]\n",
    "def makeDataSet():\n",
    "    all_data = pd.read_csv('26_8_16_PZS_Belgugung_All_Wide_NanOmited.csv',sep=\",\")\n",
    "    data_df = all_data.iloc[:,2:all_data.shape[1]]\n",
    "    max_value = np.amax(data_df.values)\n",
    "    data_df = ((data_df/max_value)*.99) + 0.01\n",
    "    train_df, test_df = splitDataToTrainAndTest(data_df,0.8)\n",
    "\n",
    "    index_of_Output = 0\n",
    "    train_df_toUse = train_df.drop(train_df.columns[[index_of_Output]],axis=1)\n",
    "    test_df_toUse = test_df.drop(test_df.columns[[index_of_Output]],axis=1)\n",
    "    test_output_df = test_df.iloc[:,index_of_Output]\n",
    "    train_output_df = train_df.iloc[:,index_of_Output]\n",
    "    \n",
    "    theData = FullDataSet()\n",
    "    theData.setTrain_input(train_df_toUse.values)\n",
    "    theData.setTest_input(test_df_toUse.values)\n",
    "    theData.setTrain_output(train_output_df.values.reshape(-1,1))\n",
    "    theData.setTest_output(test_output_df.values.reshape(-1,1))\n",
    "    return theData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def inference(x,number_input_nodes,number_hidden_nodes,number_output_nodes):\n",
    "    with tf.name_scope('input-hidden'):\n",
    "        weights = weight_variable([number_input_nodes,number_hidden_nodes])\n",
    "        bias = bias_variable([number_hidden_nodes])\n",
    "        input_hidden = tf.nn.relu(tf.matmul(x,weights)+bias)\n",
    "    \n",
    "    with tf.name_scope('hidden-output'):\n",
    "        weights = weight_variable([number_hidden_nodes,number_output_nodes])\n",
    "        bias = bias_variable([number_output_nodes])\n",
    "        hidden_output = tf.nn.relu(tf.matmul(input_hidden,weights)+bias)\n",
    "    return hidden_output\n",
    "def loss(output,real):\n",
    "    final_error = tf.square(tf.sub(real,output),name=\"myError\")\n",
    "    tf.histogram_summary(\"final_error\",final_error)\n",
    "    return final_error\n",
    "def training(loss,learning_rate):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    global_step = tf.Variable(0,name='global_step',trainable=False)\n",
    "    train_op = optimizer.minimize(loss,global_step = global_step)\n",
    "    return train_op\n",
    "def evaluation(modelOutput,correctOutput):\n",
    "    return tf.reduce_mean(tf.square(tf.sub(modelOutput,correctOutput)))\n",
    "\n",
    "def fill_feed_dict(data_set,input_pl, output_pl):\n",
    "    inputData,correctOutputData = data_set.next_batch(1)\n",
    "    feed_dict = {\n",
    "        input_pl : inputData,\n",
    "        output_pl : correctOutputData,\n",
    "    }\n",
    "    return feed_dict\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape,stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1,shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "def do_eval(sess,\n",
    "           eval_op,\n",
    "           input_pl,\n",
    "           modelOutput_pl,\n",
    "           dataset,\n",
    "           batch_size):\n",
    "    average = 0\n",
    "    steps_per_epoch = dataset.num_examples()//batch_size\n",
    "    num_examples = steps_per_epoch * batch_size\n",
    "    for step in range(steps_per_epoch):\n",
    "        feed_dict = fill_feed_dict(dataset, input_pl, modelOutput_pl)\n",
    "        average += sess.run(eval_op,feed_dict = feed_dict)\n",
    "    precision = average/num_examples\n",
    "    tf.histogram_summary(\"evaluation\",precision)\n",
    "    print('Num examples %d  Precision @ 1 %.04f'%(num_examples,precision))\n",
    "    return precision\n",
    "\n",
    "def placeholder_inputs(batch_size, number_input_nodes, number_output_nodes):\n",
    "    inputs_pl = tf.placeholder(tf.float32,shape=[batch_size,number_input_nodes])\n",
    "    outputs_pl = tf.placeholder(tf.float32,shape=[batch_size,number_output_nodes])\n",
    "    return inputs_pl, outputs_pl\n",
    "\n",
    "def run_training():\n",
    "    dataSet = makeDataSet()\n",
    "    print(dataSet.getNumberInputs())\n",
    "    #define neural network parameters\n",
    "    number_input_nodes = dataSet.getNumberInputs()\n",
    "    print(number_input_nodes)\n",
    "\n",
    "\n",
    "    number_output_nodes = 1\n",
    "    learning_rate = 0.3\n",
    "    number_hidden_nodes = 20\n",
    "    output_dir = '/Users/ahartens/Desktop/tf'\n",
    "    max_steps = 10000\n",
    "    batch_size = 1\n",
    "    \n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default(),tf.device('/cpu:0'):\n",
    "        input_pl,correctOutput_pl = placeholder_inputs(batch_size, number_input_nodes, number_output_nodes)\n",
    "        \n",
    "        modelOutput_op = inference(input_pl,\n",
    "                                  number_input_nodes,\n",
    "                                  number_hidden_nodes,\n",
    "                                  number_output_nodes)\n",
    "        loss_op = loss(modelOutput_op, correctOutput_pl)\n",
    "        \n",
    "        train_op = training(loss_op, learning_rate)\n",
    "        eval_op = evaluation(modelOutput_op, correctOutput_pl)\n",
    "        \n",
    "        summary_op = tf.merge_all_summaries()\n",
    "        \n",
    "    with tf.Session(graph = graph) as sess:\n",
    "        summary_writer = tf.train.SummaryWriter(output_dir, sess.graph)\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            myFeedDict = fill_feed_dict(dataSet.training,\n",
    "                                       input_pl,\n",
    "                                       correctOutput_pl)\n",
    "            _,loss_value,summary_str = sess.run([train_op,loss_op,summary_op],feed_dict = myFeedDict)\n",
    "            if(step%100 == 0):\n",
    "                summary_writer.add_summary(summary_str)\n",
    "                myFeedDict = fill_feed_dict(dataSet.test,\n",
    "                                           input_pl,\n",
    "                                           correctOutput_pl)\n",
    "                summary_str,eval_str = sess.run([summary_op,eval_op],feed_dict = myFeedDict)\n",
    "                summary_writer.add_summary(summary_str)\n",
    "                summary_writer.flush()\n",
    "                print(\"Loss val ist : %f\"%loss_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126\n",
      "126\n",
      "Loss val ist : 0.000003\n",
      "Loss val ist : 0.084236\n",
      "Loss val ist : 0.025115\n",
      "Loss val ist : 0.000163\n",
      "Loss val ist : 0.015570\n",
      "Loss val ist : 0.001472\n",
      "Loss val ist : 0.023772\n",
      "Loss val ist : 0.077682\n",
      "Loss val ist : 0.008969\n",
      "Loss val ist : 0.028815\n",
      "Loss val ist : 0.000349\n",
      "Loss val ist : 0.017021\n",
      "Loss val ist : 0.005024\n",
      "Loss val ist : 0.024975\n",
      "Loss val ist : 0.004576\n",
      "Loss val ist : 0.021186\n",
      "Loss val ist : 0.000255\n",
      "Loss val ist : 0.007840\n",
      "Loss val ist : 0.008767\n",
      "Loss val ist : 0.007937\n",
      "Loss val ist : 0.000023\n",
      "Loss val ist : 0.003697\n",
      "Loss val ist : 0.048999\n",
      "Loss val ist : 0.001234\n",
      "Loss val ist : 0.011455\n",
      "Loss val ist : 0.002794\n",
      "Loss val ist : 0.000104\n",
      "Loss val ist : 0.000171\n",
      "Loss val ist : 0.000001\n",
      "Loss val ist : 0.001800\n",
      "Loss val ist : 0.002434\n",
      "Loss val ist : 0.007048\n",
      "Loss val ist : 0.015392\n",
      "Loss val ist : 0.039006\n",
      "Loss val ist : 0.000611\n",
      "Loss val ist : 0.002011\n",
      "Loss val ist : 0.010678\n",
      "Loss val ist : 0.000184\n",
      "Loss val ist : 0.004961\n",
      "Loss val ist : 0.006789\n",
      "Loss val ist : 0.002756\n",
      "Loss val ist : 0.014068\n",
      "Loss val ist : 0.001801\n",
      "Loss val ist : 0.000992\n",
      "Loss val ist : 0.000039\n",
      "Loss val ist : 0.001643\n",
      "Loss val ist : 0.003301\n",
      "Loss val ist : 0.078450\n",
      "Loss val ist : 0.028480\n",
      "Loss val ist : 0.001755\n",
      "Loss val ist : 0.000456\n",
      "Loss val ist : 0.002369\n",
      "Loss val ist : 0.000654\n",
      "Loss val ist : 0.000237\n",
      "Loss val ist : 0.017768\n",
      "Loss val ist : 0.008917\n",
      "Loss val ist : 0.008294\n",
      "Loss val ist : 0.006971\n",
      "Loss val ist : 0.006847\n",
      "Loss val ist : 0.004094\n",
      "Loss val ist : 0.108956\n",
      "Loss val ist : 0.023018\n",
      "Loss val ist : 0.006659\n",
      "Loss val ist : 0.000641\n",
      "Loss val ist : 0.035686\n",
      "Loss val ist : 0.003902\n",
      "Loss val ist : 0.010060\n",
      "Loss val ist : 0.012723\n",
      "Loss val ist : 0.005336\n",
      "Loss val ist : 0.007531\n",
      "Loss val ist : 0.000262\n",
      "Loss val ist : 0.000744\n",
      "Loss val ist : 0.019133\n",
      "Loss val ist : 0.000948\n",
      "Loss val ist : 0.024584\n",
      "Loss val ist : 0.000012\n",
      "Loss val ist : 0.005252\n",
      "Loss val ist : 0.000009\n",
      "Loss val ist : 0.010224\n",
      "Loss val ist : 0.023942\n",
      "Loss val ist : 0.021965\n",
      "Loss val ist : 0.004201\n",
      "Loss val ist : 0.020694\n",
      "Loss val ist : 0.020780\n",
      "Loss val ist : 0.000779\n",
      "Loss val ist : 0.036756\n",
      "Loss val ist : 0.017039\n",
      "Loss val ist : 0.000163\n",
      "Loss val ist : 0.003819\n",
      "Loss val ist : 0.008099\n",
      "Loss val ist : 0.000506\n",
      "Loss val ist : 0.023302\n",
      "Loss val ist : 0.002903\n",
      "Loss val ist : 0.001800\n",
      "Loss val ist : 0.002213\n",
      "Loss val ist : 0.000417\n",
      "Loss val ist : 0.000003\n",
      "Loss val ist : 0.014048\n",
      "Loss val ist : 0.014065\n",
      "Loss val ist : 0.016659\n"
     ]
    }
   ],
   "source": [
    "run_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [tensorflow]",
   "language": "python",
   "name": "Python [tensorflow]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
