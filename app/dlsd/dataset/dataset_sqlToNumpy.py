import pandas as pd
import numpy as np
from . import dataset_helpers as dsh
from dlsd import Common as c

def sqlToNumpy_pivotAndSmooth(inputFile,specifiedSensors,sensorEfficiency,window = 50):
    '''
        First step of all further analyses :
        Long/narrow dataset from SQL is made wide (one column per sensor, time stamps are rows)
        Format of input file must be [S_IDX,ZEIT,wert]
        Args : 
            inputFile :         Path to csv file generated by sql
            specifiedSensors :  numpy array of sensor Ids that should be used. If present then inefficients sensors not removed
            sensorEfficiency :  cutoff for which sensors are removed. If specifiedSensors not None, this is irrelevant
        Return :
            data_wide :         Pandas dataframe containing desired data
    '''
    all_data = pd.read_csv(inputFile,sep=",")
    c.debugInfo(__name__,"Read input SQL file with shape : (%d, %d)"%(all_data.shape[0],all_data.shape[1]))

    if (specifiedSensors is not None):
        c.debugInfo(__name__,"%d Sensors specified, getting indices from"%specifiedSensors.shape[0])
        sensorIndices = np.where(all_data.iloc[:,0].values==specifiedSensors)[1]
        all_data = all_data.iloc[sensorIndices,:]

    data_wide_all = all_data.pivot(index='ZEIT', columns='S_IDX', values='wert')
    c.debugInfo(__name__,"Pivoted input shape : (%d, %d)"%(data_wide_all.shape[0],data_wide_all.shape[1]))

    # make table containing only efficient sensors (only columns with efficiency >sensorEfficiency nan are used)
    if (specifiedSensors is None):
        data_wide = removeInefficientSensors(data_wide_all,sensorEfficiency)
    else:
        data_wide = data_wide_all

    # do the rolling mean
    data_wide = data_wide.rolling(window,min_periods =1).mean()
    c.debugInfo(__name__,"Calculated the rolling average using window %d"%window)

    return data_wide, data_wide_all.shape[0]

def removeInefficientSensors(data_wide_all,sensorEfficiency):
    #count the number of times each column has an 'na' value
    counts = np.zeros((data_wide_all.shape[1],1))
    for i in range(0,data_wide_all.shape[1]):
        counts[i] = len(np.where(np.isnan(data_wide_all.iloc[:,i]))[0])

    # calculate the efficiency of the sensor
    sensorsToEfficiency = pd.DataFrame(np.zeros((3,counts.shape[0])))
    sensorsToEfficiency.iloc[0,:]=data_wide_all.columns.values.reshape(1,-1)
    sensorsToEfficiency.iloc[2,:]=1-counts.reshape(1,-1)/(data_wide_all.shape[0])
    sensorsToEfficiency.iloc[1,:]=counts.reshape(1,-1)
    
    # make table containing only efficient sensors (only columns with <10 nan are used)
    efficientSensorIndices = np.where(sensorsToEfficiency.iloc[2,:].values>sensorEfficiency)
    data_wide = data_wide_all.iloc[:,efficientSensorIndices[0]]
    data_wide.shape
    c.debugInfo(__name__,"Data where sensors have efficiency > %.2f : (%d, %d)"%(sensorEfficiency,data_wide.shape[0],data_wide.shape[1]))
    c.debugInfo(__name__,"There are %d sensors in total, but only %d have efficiency > %.2f"%(data_wide_all.shape[1], data_wide.shape[1],sensorEfficiency))
    return data_wide

def sqlToNumpy_allSensorsInAllOutWithTimeOffset(inputFile,
                                                outputFilePath, 
                                                timeOffset = 15, 
                                                sensorEfficiency = .98,
                                                specifiedSensors = None,
                                                sensorsOutputPath = ""):
    '''
        Creates data from SQL output for using in machine learning model. (input/output contained in one row)
        Then the table is doubled and concatenated to itself, offset by time (timeOffset)

        Original found in analysis > 10_16 > 24_10_sqlToNumpy_allInputallOutput_timeOffset

        Results in a table where each row has inpu
        

        Args : 
            inputFile :         Path to csv file generated by sql. CSV contains 3 columns 
                                (sensor name, time stamp, value (eg belegung))
            outputFilePath :        Path to output file.
            saveOutputFile :          Boolean, if True the file is written to outputFile
            timeOffset :        Int value of minutes offset between input/output
        Return :
            theData :       FullDataSet object from dataset_helpers containing two DataSet 
                            objects containing two numpy arrays(input/target)
    '''


    data_wide, originalSize = sqlToNumpy_pivotAndSmooth(inputFile,specifiedSensors,sensorEfficiency)
    
    # print out sensors used for this analysis to file as long list. Used later for specifying sensors
    if (specifiedSensors is None):
        c.debugInfo(__name__,"Saving Sensors list to %s"%(sensorsOutputPath))
        sensorsUsed = pd.DataFrame(data_wide.columns.values.reshape((data_wide.columns.values.shape[0],-1)))
        sensorsUsed.to_csv(sensorsOutputPath,index=False,header=False)
    else:
        c.debugInfo(__name__,"Sensors list is specified : not resaving sensors")
    # each row has two time points for example :
    # [All sensors at timepoint 0] [All sensors at timepoint 0+timeOffset]
    # set the offset time to 'output'

    # make empty data frame to fit this data
    newShape = (data_wide.shape[0],data_wide.shape[1]*2)

    # create empty array filled with nan
    data_final = pd.DataFrame(np.zeros(newShape))
    data_final[:] = np.NAN

    # first fill 'input' data
    data_final.iloc[timeOffset:data_wide.shape[0],0:data_wide.shape[1]] = data_wide.iloc[0:data_wide.shape[0]-timeOffset,0:data_wide.shape[1]].values

    # then fill 'output' data : time at x + timeOffset
    c.debugInfo(__name__,"Creating output data at time offset %d"%timeOffset)
    data_final.iloc[:,data_wide.shape[1]:2*data_wide.shape[1]] = data_wide.iloc[:,:].values

    # set column names
    colNames = data_wide.columns.values.reshape(1,-1)
    colNames_final = np.concatenate((colNames,colNames),axis=1)
    data_final.columns = colNames_final[0,:]

    # drop all rows with NaN somewhere
    c.debugInfo(__name__,"Removing rows with >0 NaNs")
    data_final_naDropped = data_final.dropna()
    c.debugInfo(__name__,"From %d total timepoints, %d are being used (%.2f)"%(originalSize,data_final_naDropped.shape[0],(data_final_naDropped.shape[0]/originalSize)))

    #data_final.to_csv("/Users/ahartens/Desktop/Temporary/24_10_16_wideTimeSeriesBelegung.csv")
    if (outputFilePath is not None):
        c.debugInfo(__name__,"Saving processed file to %s"%(outputFilePath))
        data_final_naDropped.to_csv(outputFilePath,index=False)

    return data_final_naDropped

def sqlToNumpy_allSensorsWithAllTimeOffsetAsInput(inputFile,
                                                outputFilePath, 
                                                saveOutputFile = False, 
                                                timeOffset = 15, 
                                                sensorEfficiency = .98,
                                                window = 50):
    '''
        16_11_2 Analysis 3
        Creates data from SQL output for using in machine learning model. (input/output contained in one row)
        Long/narrow dataset from SQL is made wide (one column per sensor, time stamps are rows)
        Then the table is doubled and concatenated to itself, offset by time (timeOffset)

        Original found in analysis > 10_16 > 24_10_sqlToNumpy_allInputallOutput_timeOffset

        Results in a table where each row has inpu
        

        Args : 
            inputFile :         Path to csv file generated by sql. CSV contains 3 columns 
                                (sensor name, time stamp, value (eg belegung))
            outputFilePath :        Path to output file.
            saveOutputFile :          Boolean, if True the file is written to outputFile
            timeOffset :        Int value of minutes offset between input/output
        Return :
            theData :       FullDataSet object from dataset_helpers containing two DataSet 
                            objects containing two numpy arrays(input/target)
    '''
    timeOffset = 15
    all_data = pd.read_csv(inputFile,sep=",")
    c.debugInfo(__name__,"Read input SQL file with shape : (%d, %d)"%(all_data.shape[0],all_data.shape[1]))

    data_wide_all = all_data.pivot(index='ZEIT', columns='S_IDX', values='wert')
    c.debugInfo(__name__,"Pivoted input shape : (%d, %d)"%(data_wide_all.shape[0],data_wide_all.shape[1]))

    # make table containing only efficient sensors (only columns with efficiency >sensorEfficiency nan are used)

    #count the number of times each column has an 'na' value
    counts = np.zeros((data_wide_all.shape[1],1))
    for i in range(0,data_wide_all.shape[1]):
        counts[i] = len(np.where(np.isnan(data_wide_all.iloc[:,i]))[0])

    # calculate the efficiency of the sensor
    sensorsToEfficiency = pd.DataFrame(np.zeros((3,counts.shape[0])))
    sensorsToEfficiency.iloc[0,:]=data_wide_all.columns.values.reshape(1,-1)
    sensorsToEfficiency.iloc[2,:]=1-counts.reshape(1,-1)/(data_wide_all.shape[0])
    sensorsToEfficiency.iloc[1,:]=counts.reshape(1,-1)
    
    # make table containing only efficient sensors (only columns with <10 nan are used)
    efficientSensorIndices = np.where(sensorsToEfficiency.iloc[2,:].values>sensorEfficiency)
    data_wide = data_wide_all.iloc[:,efficientSensorIndices[0]]
    data_wide.shape
    c.debugInfo(__name__,"Data where sensors have efficiency > %.2f : (%d, %d)"%(sensorEfficiency,data_wide.shape[0],data_wide.shape[1]))
    c.debugInfo(__name__,"There are %d sensors in total, but only %d have efficiency > %.2f"%(data_wide_all.shape[1], data_wide.shape[1],sensorEfficiency))

    # do the rolling mean
    rollingMean = pd.rolling_mean(data_wide,window)
    data_wide = rollingMean.iloc[window:data_wide.shape[0],:]
    c.debugInfo(__name__,"Calculated the rolling average using window %d"%window)
    # each row has two time points for example :
    # [All sensors at timepoint 0][All sensors at timepoint 2][All sensors at timepoint ...]  Output : [All sensors at timepoint 0+timeOffset]
    # set the offset time to 'output'


    # make empty data frame to fit this data
    newShape = (data_wide.shape[0]+timeOffset,data_wide.shape[1]*timeOffset)

    # create empty array filled with nan
    data_final = pd.DataFrame(np.zeros(newShape))
    data_final[:] = np.NAN

    # first fill 'input' data
    #data_final.iloc[0:data_wide.shape[0],0:data_wide.shape[1]] = data_wide.iloc[0:data_wide.shape[0],0:data_wide.shape[1]].values

    for i in range(0,timeOffset):
        data_final.iloc[i:data_wide.shape[0]+i,data_wide.shape[1]*i:data_wide.shape[1]*(i+1)] = data_wide.iloc[:,0:data_wide.shape[1]].values

    # then fill 'output' data : time at x + timeOffset
    #data_final.iloc[timeOffset:data_wide.shape[0]+timeOffset,data_wide.shape[1]:2*data_wide.shape[1]] = data_wide.iloc[:,0:data_wide.shape[1]].values
    # set column names
    colNames = data_wide.columns.values.reshape(1,-1)
    colNames_final = colNames
    for i in range(1,timeOffset):
        colNames_final = np.concatenate((colNames_final,colNames),axis=1)
    data_final.columns = colNames_final[0,:]
    # drop all rows with NaN somewhere
    c.debugInfo(__name__,"Removing rows with >0 NaNs")
    data_final_naDropped = data_final.dropna()
    c.debugInfo(__name__,"From %d total timepoints, %d are being used (%.2f)"%(data_wide_all.shape[0],data_final_naDropped.shape[0],(data_final_naDropped.shape[0]/data_wide_all.shape[0])))
    print(data_final_naDropped.shape)

    #data_final.to_csv("/Users/ahartens/Desktop/Temporary/24_10_16_wideTimeSeriesBelegung.csv")
    if (saveOutputFile == True):
        c.debugInfo(__name__,"Saving processed file to %s"%(outputFilePath))
        data_final_naDropped.to_csv(outputFilePath,index=False)

    return data_final_naDropped

